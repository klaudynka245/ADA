\documentclass[12pt]{mwart}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{mathtools, amsthm, amssymb, amsmath}
\usepackage[plmath]{polski}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{rotating}
\DeclareMathOperator{\E}{\text{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\date{\today}
\title{Sprawozdanie 2}
\author{Olimpia Pozauć, Bogdan Banasiak, Piotr Zieleń}
<<echo=F>>=
pdf.options(encoding="CP1250")
@
\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
\tableofcontents
\newpage

\section{Wstęp}

Celem sprawozdania jest przedstawienie rozwiązań oraz wniosków z rozwiązywanych podczas zajęć labolatoryjnych kolejnych list zadań.

\section{Lista 5-6-7}

W tej liście przeanalizujemy zadania od 2. do 5., ponieważ pierwsze zadanie było jedynie ćwiczeniowe i zostało zrealizowane na zajęciach.

W pierwszym kroku zaimportowaliśmy biblioteki, które przydały się do rozwiązywanych zadań:
<<warning=F, message=F>>=
library(binom)
library(stats)
@

\subsection{Zadanie 2.}\label{section:2.1}

Celem zadania 2. jest przeanalizowanie wyników ankiety, którą sporządzono w kilku losowo wybranych aptekach. Klienci w różnym wieku zostali poproszeni o wskazanie najczęściej kupowanego przez nich leku przeciwbólowego. 
Dane zostały przestawione na tabeli (\ref{tab:tabela1}).
\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ccccc}
			\hline
			& \multicolumn{2}{c}{Wiek ankietowanych} & & \\\hline
			Lek & do lat 35 & od 36 do 55 & powyżej 55 & Suma \\\hline
			Ibuprom & 35 & 0 & 0 & 35 \\
			Apap & 22 & 22 & 0 & 44 \\
			Paracetamol & 15 & 15 & 15 & 45 \\
			Ibuprofen & 0 & 40 & 10 & 50 \\
			Panadol & 18 & 3 & 5 & 26 \\\hline
			Suma & 90 & 80 & 30 & 200 \\\hline
		\end{tabular}
	\end{center}
	\caption{Tablica dwudzielcza zależności wyboru leku Panadol lub innego leku, od wieku}
	\label{tab:tabela1}
\end{table}

\noindent Na podstawie tych danych, należało zweryfikować hipotezy opisane w (\ref{section:a}), (\ref{section:b}), (\ref{section:c}), (\ref{section:d}) na poziomie istotności $\alpha=0.05$, podać wartość poziomu krytycznego i sformułować odpowiedź.

Wykonamy trzy testy: \verb|binom.test| oraz \verb|prop.test| z oraz bez uwzględnienia poprawki ciągłości Yatesa.

\subsubsection{(a)}\label{section:a}
\textbf{$H_0$:} prawdopodobieństwo, że losowo wybrana osoba z badanej populacji w przypadku bólu zażywa Apap jest mniejsze bądź równe $\frac{1}{4}$

\textbf{$H_1$:} prawdopodobieństwo, że losowo wybrana osoba z badanej populacji w przypadku bólu zażywa Apap jest większe od $\frac{1}{4}$
<<>>=
apap <- 44
n <- 200
@
<<>>=
binom.test(apap, n, p=1/4, alternative="g")$p.value
@

<<>>=
prop.test(apap, n, p=1/4, alternative="g", correct=T)$p.value
@

<<>>=
prop.test(apap, n, p=1/4, alternative="g", correct=F)$p.value
@

W każdym z powyższych testów uzyskaliśmy p{\dywiz}wartość znacznie większą niż założony poziom istotności $\alpha$, więc nie mamy podstaw do odrzucenia hipotezy zerowej $H_0$, czyli prawdopodobieństwo, że losowo wybrana osoba z badanej populacji w przypadku bólu zażywa Apap, jest mniejsze lub równe $\frac{1}{4}$.

\subsubsection{(b)}\label{section:b}
\textbf{$H_0$:} prawdopodobieństwo, że losowo wybrana osoba z badanej populacji w przypadku bólu zażywa Apap jest równe $\frac{1}{2}$

\textbf{$H_1$:} prawdopodobieństwo, że losowo wybrana osoba z badanej populacji w przypadku bólu zażywa Apap jest różne od $\frac{1}{2}$

<<>>=
apap <- 44
n <- 200
@

<<>>=
binom.test(apap, n, p=1/2, alternative="t")$p.value
@

<<>>=
prop.test(apap, n, p=1/2, alternative="t", correct=T)$p.value
@

<<>>=
prop.test(apap, n, p=1/2, alternative="t", correct=F)$p.value
@

W każdym z przeprowadzonych testów uzyskaliśmy p{\dywiz}wartość praktycznie równą zero, więc możemy założyć, że hipoteza zerowa $H_0$ jest nieprawdziwa, czyli zakładamy, że prawdziwa jest hipoteza alternatywna $H_1$, czyli prawdopodobieństwo, że losowo wybrana osoba z badanej populacji, w przypadku bólu zażywa Apap, jest różne od $\frac{1}{2}$.

\subsubsection{(c)}\label{section:c}
\textbf{$H_0$:} prawdopodobieństwo, że losowo wybrana osoba z badanej populacji w przypadku bólu zażywa Ibuprom jest większe bądź równe $\frac{1}{5}$

\textbf{$H_1$:} prawdopodobieństwo, że losowo wybrana osoba z badanej populacji w przypadku bólu zażywa Ibuprom jest mniejsze od $\frac{1}{5}$

<<>>=
ibuprom <- 35
n <- 200
@

<<>>=
binom.test(ibuprom, n, p=1/5, alternative="l")$p.value
@

<<>>=
prop.test(ibuprom, n, p=1/5, alternative="l", correct=T)$p.value
@

<<>>=
prop.test(ibuprom, n, p=1/5, alternative="l", correct=F)$p.value
@

W każdym z przeprowadzonych testów uzyskaliśmy p{\dywiz}wartość większą niż założony poziom istotności $\alpha$, więc możemy założyć, że hipoteza zerowa $H_0$ jest prawdziwa, czyli prawdopodobieństwo, ze losowo wybrana osoba z badanej populacji, w przypadku bólu zażywa Ibuprom, jest większe bądź równe $\frac{1}{5}$.

\subsubsection{(d)}\label{section:d}
W podpunkcie (d) powtarzamy pozostałe podpunkty, ale bierzemy pod uwagę tylko grupę wiekową do lat 35.

\textbf{(a)}

\textbf{$H_0$:} prawdopodobieństwo, że losowo wybrana osoba, do 35 lat, z badanej populacji w przypadku bólu zażywa Apap jest mniejsze bądź równe $\frac{1}{4}$

\textbf{$H_1$:} prawdopodobieństwo, że losowo wybrana osoba, do 35 lat, z badanej populacji w przypadku bólu zażywa Apap jest większe od $\frac{1}{4}$

<<>>=
apap_35 <- 22
n_35 <- 90
@

<<>>=
binom.test(apap_35, n_35, p=1/4, alternative="g")$p.value
@

<<>>=
prop.test(apap_35, n_35, p=1/4, alternative="g",
				correct=T)$p.value
@

<<>>=
prop.test(apap_35, n_35, p=1/4, alternative="g",
				correct=F)$p.value
@

\textbf{(b)}

\textbf{$H_0$:} prawdopodobieństwo, że losowo wybrana osoba, do 35 lat, z badanej populacji w przypadku bólu zażywa Apap jest równe $\frac{1}{2}$

\textbf{$H_1$:} prawdopodobieństwo, że losowo wybrana osoba, do 35 lat, z badanej populacji w przypadku bólu zażywa Apap jest różne od $\frac{1}{2}$

<<>>=
apap_35 <- 22
n_35 <- 90
@

<<>>=
binom.test(apap_35, n_35, p=1/2, alternative="t")$p.value
@

<<>>=
prop.test(apap_35, n_35, p=1/2, alternative="t",
				correct=T)$p.value
@

<<>>=
prop.test(apap_35, n_35, p=1/2, alternative="t",
				correct=F)$p.value
@

\textbf{(c)}

\textbf{$H_0$:} prawdopodobieństwo, że losowo wybrana osoba, do 35 lat, z badanej populacji w przypadku bólu zażywa Ibuprom jest większe bądź równe $\frac{1}{5}$

\textbf{$H_1$:} prawdopodobieństwo, że losowo wybrana osoba, do 35 lat, z badanej populacji w przypadku bólu zażywa Ibuprom jest mniejsze od $\frac{1}{5}$

<<>>=
ibuprom_35 <- 35
n_35 <- 90
@

<<>>=
binom.test(ibuprom_35, n, p=1/5, alternative="l")$p.value
@

<<>>=
prop.test(ibuprom_35, n, p=1/5, alternative="l",
				correct=T)$p.value
@

<<>>=
prop.test(ibuprom_35, n, p=1/5, alternative="l",
				correct=F)$p.value
@

Dla każdego z rozważanych przypadków \pauza \textbf{(a)}, \textbf{(b)}, \textbf{(c)} uzyskaliśmy p{\ppauza}wartości, dla których możemy wyciągnąć podobne wnioski, jak dla badania dla całej badanej populacji.

\begin{itemize}[label=$\bullet$]
\item W przypadku \textbf{(a)} nie ma podstaw do odrzucenia hipotezy zerowej $H_0$,
\item W przypadku \textbf{(b)} możemy założyć, że hipoteza zerowa $H_0$ jest fałszywa, więc przyjmujemy hipotezę alternatywaną $H_1$
\item W przypadku \textbf{(c)} nie ma podstaw do odrzucenia hipotezy zerowej $H_0$.
\end{itemize}

\subsection{Zadanie 3.}

Celem zadania jest weryfikacja hipotezy na poziomie istotności $\alpha$ = 0.05, że prawdopodobieństwo, że osoba do lat 35 zażywa Panadol jest równe prawdopodobieństwu, że osoba od 36 lat do 55 lat
zażywa Panadol. W zadaniu wykorzystamy test Fishera oraz opierać się będziemy na danych z tabeli (\ref{tab:tabela1}). 

Następnie postaramy się opowiedzieć na pytanie, czy na podstawie uzyskanego wyniku można odrzucić hipotezę o niezależności wyboru leku Panadol w leczeniu bólu od wieku, przy uwzględnieniu tylko dwóch grup wiekowych wymienionych wyżej?

W pierwszym kroku przedstawimy tablicę dwudzielczą, dla danych \pauza tabela (\ref{tab:tabela3}):

<<>>=
data <- matrix(c(18, 72, 3, 77), nrow=2, ncol=2)
@

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			& \multicolumn{2}{|c|}{Wiek ankietowanych}\\\hline
			& do lat 35 & od 36 do 55 lat \\\hline
			Panadol & \Sexpr{data[1, 1]} & \Sexpr{data[1, 2]} \\\hline
			Inny lek & \Sexpr{data[2, 1]} & \Sexpr{data[2, 2]}\\\hline
		\end{tabular}
	\end{center}
	\caption{Dane do zadania 2.}
	\label{tab:tabela3}
\end{table}

Hipotezy testu Fishera:
\begin{itemize}[label=$\bullet$]
	\item $H_0$: wiek i lek są niezależne
	\item $H_1$: wiek i lek nie są niezależne.
\end{itemize}

<<>>=
fisher.test(data)$p.value
@

Uzyskaliśmy p{\dywiz}wartość mniejszą niż założony poziom istotności $\alpha=0.05$, więc mamy podstawy, aby odrzucić hipotezę o niezależności wyboru leku Panadol w leczeniu bólu od wieku, przy uwzględnieniu dwóch grup wiekowych (od lat 35 i od 36 do 55 lat) \pauza zakładamy, że wiek i lek w badanej populacji nie są niezależne.

\subsection{Zadanie 4.}

W zadaniu 4. należało zweryfikować hipotezę o niezależności stopnia zadowolenia z pracy i wynagrodzenia, korzystając z funkcji \verb|chisq.test|. Dane do przeanalizowania przedstawiliśmy w tabeli (\ref{tab:tabela2}). Zadanie dotyczy weryfikacji hipotezy zerowej na poziomie istotności $\alpha$ = 0.05. 

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{cccccc}
			\hline
			& \multicolumn{2}{c}{Stopień zadowolenia z pracy} & & \\\hline
			Wynagrodzenie & b. niezadow. & niezadow. & zadow. & b. zadow. & Suma \\\hline
			do 6000 & 20 & 24 & 80 & 82 & 206 \\
			6000-15000 & 22 & 38 & 104 & 125 & 289 \\
			15000-25000 & 13 & 28 & 81 & 113 & 235 \\
			powyżej 25000 & 7 & 18 & 54 & 92 & 171 \\\hline
			Suma & 62 & 108 & 319 & 412 & 901 \\\hline
		\end{tabular}
	\end{center}
	\caption{Dane do zadania 4. i 5.}
	\label{tab:tabela2}
\end{table}

Hipotezy testu chi{\dywiz}kwadrat:
\begin{itemize}[label=$\bullet$]
	\item $H_0$: Stopień zadowolenia z pracy i wynagrodzenie są niezależne
	\item $H_1$: Stopień zadowolenia z pracy i wynagrodzenie nie sa niezależne
\end{itemize}

<<>>=
(data2 <- matrix(c(20, 22, 13, 7,
      		24, 38, 28, 18,
      		80, 104, 81, 54,
      		82, 125, 113, 92),
      		nrow=4, ncol=4))
@

<<>>=
chisq.test(data2)$p.value
@

Uzyskalismy p{\dywiz}wartość większą niż założony poziom istotności, więc nie mamy podstaw do odrzucenia hipotezy o niezależności, czyli zakładamy, że stopień zadowolenia z pracy i wynagrodzenie są niezależne.

\subsection{Zadanie 5.}
Celem zadania było napisanie deklaracji funkcji, która dla danych w tablicy dwudzielczej oblicza wartość poziomu krytycznego (p-value) w asymptotycznym teście niezależności opartym na ilorazie wiarogodności, dla danych z tabeli (\ref{tab:tabela2}). Dodatkowo należało podać hipotezy dla asymptotycznego testu niezależności, opartym na ilorazie wiarogodności, wyznaczyć p{\dywiz}wartość na podstawie napisanej funkcji i wyciągnąć wniosek.

Hipotezy dla asymptotycznego testu niezależności:
\begin{itemize}[label=$\bullet$]
	\item $H_0$: Stopień zadowolenia z pracy i wynagrodzenie są niezależne
	\item $H_1$: Stopień zadowolenia z pracy i wynagrodzenie nie sa niezależne
\end{itemize}

<<>>=
(Data <- matrix(c(20, 22, 13, 7,
		24, 38, 28, 18,
		80, 104, 81, 54,
		82, 125, 113, 92),
	nrow=4, ncol=4))
@


<<>>=
funkcja <- function(data){
	n <- sum(data)
	N <- matrix(rowSums(data), nrow=nrow(data),
			ncol=nrow(data))
	R <- t(matrix(colSums(data), nrow=ncol(data),
			ncol=ncol(data)))
	G2 <- prod(((N*R)/(n * data)) ^ data)
	G2 <- -2*log(G2)
	1-pchisq(G2, (length(N[1,])-1)*(length(R[1,])-1))
}
@

Następnie obliczamy wartość krytyczną dla danych z zadania 4. \pauza tabela (\ref{tab:tabela2}), korzystając z napisanej funkcji:

<<>>=
funkcja(Data)
@

Podobnie jak w teście chi{\dywiz}kwadrat \ppauza uzyskaliśmy p{\dywiz}wartość większą niż zadany poziom istotności $\alpha$, więc nie mamy podstaw do odrzucenia hipotezy zerowej \pauza zakładamy, że stopień zadaowolenia z pracy i wynagrodzenie są niezależne.

\section{Lista 8\_9}
Zaimportowaliśmy biblioteki, które przydały się nam do rozwiązania zadań:
<<warning=F, message=F>>=
library(stats)
library(DescTools)
library(psych)
library(expm)
library(ca)
#library(matlib)
@
Dodatkowo, w każdym z poniższych zadań będziemy przeprowadzać analizę korespondencji,
dla której kod zamieszczamy poniżej:
<<>>=
analiza.kores <- function(Dane, title_='', columnsName='', 
                          rowsName='', dispMatrix=FALSE) {
  nc <-  colSums(Dane)
  nr <- rowSums(Dane)
  n <- sum(nr)
  P <- Dane / n
  c <- nc / n
  r <- nr / n
  Dc <- diag(c)
  Dr <- diag(r)
  A <-inv(Dr ^ (1/2)) %*% (P- r %*% t(c)) %*% inv(Dc ^ (1/2))
  U <-  svd(A)$u
  V <-  svd(A)$v
  Gamma <-  diag(svd(A)$d)
  dispF <- inv(Dr^(1/2)) %*% U %*% Gamma
  dispG <- inv(Dc^(1/2)) %*% V %*% Gamma
  F_ <- dispF[,1:2] 
  G <- dispG[,1:2]
  gam <- svd(A)$d ^ 2

  # Creating labels
  if (rowsName=='') {
    rowsName <- c()
    for (i in 1:nrow(Dane)) {
      str_ = paste('row', i, sep = '')
      rowsName <- append(rowsName, str_)
    }
  }
  
  if (columnsName=='') {
    columnsName <- c()
    for (i in 1:ncol(Dane)) {
      str_ = paste('col', i, sep = '')
      columnsName <- append(columnsName, str_)
    }
  }
  xlabel <- paste("Dimension 1 (", round(sum(gam[1])/sum(gam), 3) * 100, '%)', 
                  sep = '')
  ylabel <- paste("Dimension 2 (", round(sum(gam[2])/sum(gam), 3) * 100, '%)', 
                  sep = '')
  plot(F_, col='blue', pch = 19,
       main=title_,
       xlab=xlabel,
       ylab=ylabel,
       xlim=c(min(c(min(F_), min(G[,1])))-0.01, max(c(max(F_[,1]), max(G[,1])))),
       ylim=c(min(c(min(F_[,2]), min(G[,2])))-0.001, max(c(max(F_[,2]), max(G[,2])))))
  abline(h=0)
  abline(v=0)
  text(F_, rowsName, pos=2)
  
  points(G, col = "red", pch = 19)
  text(G, columnsName, pos=2)
  grid(lty=6, col="grey")
  
  if (dispMatrix) {
    c(sum(gam), sum(gam[1:2])/sum(gam), P, A, dispF, dispG, c, r)
  }
}
@

\subsection{Zadanie 1.}\label{section:3.1}
W tym zadaniu należało przeprowadzić test o niezależności, wyznaczyć miary współzmienności, korzystając z własnej funkcji i porównać otrzymane wyniki w funkcjami wbudowanymi w pakiecie R,  oraz przeprowadzić analizę korespondencji na podstawie danych z przykładu 1. na wykładzie 7.
\subsubsection{Dane z przykładu 1. z wykładu 7.}
\emph{W ramach projektu ,,Współczesne problemy ekologiczne świata", wród studentów polskich uczelni przeprowadzono badania ankietowe na temat ich świadomości ekologicznej.}\newline\noindent
\emph{Między innymi zadano im pytanie dotyczące segregacji śmieci, na które mogli wybrać jedną z czterech poniższych odpowiedzi:}
\begin{enumerate}[label=(\Alph*)]
	\item \emph{Segreguję śmieci, ponieważ jest to korzystne dla środowiska;}
	\item \emph{Segreguję śmieci ponieważ taki jest wymóg ustalony;}
	\item \emph{Segreguję śmieci, ponieważ wszyscy tak robią;}
	\item \emph{Nie segreguję śmieci.}
\end{enumerate}
Poniżej przedstawiliśmy wyniki ankiety:
\begin{itemize}[label=$\bullet$]
	\item Tablica dwudzielcza dla kategorii \emph{Segregacja} i \emph{Wiek}, (dla połączonych kategorii wiekowych):
<<>>=
m1a <- matrix(c(888,369,50,457,263,95,10,99,208,29,2,
			44,78,9,0,19,1,0,0,4),
				nrow=5,byrow=TRUE)
dimnames(m1a) <- list(c("18-25","26-35","36-45",
			"46-59","60+"),
			c("A","B","C","D"))
@
\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			& A & B & C & D \\\hline
			18\dywiz25 & \Sexpr{m1a[1, 1]}& \Sexpr{m1a[1, 2]}& \Sexpr{m1a[1, 3]}& \Sexpr{m1a[1, 4]} \\\hline
			26\dywiz35 & \Sexpr{m1a[2, 1]}& \Sexpr{m1a[2,2]}& \Sexpr{m1a[2,3]}& \Sexpr{m1a[2,4]} \\\hline
			36\dywiz45 & \Sexpr{m1a[3, 1]}& \Sexpr{m1a[3,2]}& \Sexpr{m1a[3,3]}& \Sexpr{m1a[3,4]} \\\hline
			46\dywiz59 & \Sexpr{m1a[4, 1]}& \Sexpr{m1a[4, 2]}& \Sexpr{m1a[4,3]}& \Sexpr{m1a[4,4]} \\\hline
			60+ & \Sexpr{m1a[5, 1]}& \Sexpr{m1a[5, 2]}& \Sexpr{m1a[5, 3]}& \Sexpr{m1a[5,4]} \\\hline
		\end{tabular}
	\end{center}
	\caption{Tablica dwudzielcza dla kategorii \emph{Segregacja} i \emph{Wiek}, (dla połączonych kategorii wiekowych)}
	\label{tab:tabela4}
\end{table}

	\item Tablica dwudzielcza dla kategorii \emph{Segregacja} i \emph{Miejsce zamieszkania}:

<<>>=
m2a <- matrix(c(505,202,19,136,240,77,14,88,181,
		63,8,105,512,159,21,294),nrow=4,byrow=TRUE)
dimnames(m2a) <- list(c("Wies","Miasto do 20 tys.",
	"Miasto 20-50 tys.","Miasto pow. 50 tys."),
			c("A","B","C","D"))
@

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			& A & B & C & D \\\hline
			Wieś & \Sexpr{m2a[1, 1]}& \Sexpr{m2a[1, 2]}& \Sexpr{m2a[1, 3]}& \Sexpr{m2a[1, 4]} \\\hline
			Miasto do 20 tys. & \Sexpr{m2a[2, 1]}& \Sexpr{m2a[2,2]}& \Sexpr{m2a[2,3]}& \Sexpr{m2a[2,4]} \\\hline
			Miasto 20\dywiz50 tys. & \Sexpr{m2a[3, 1]}& \Sexpr{m2a[3,2]}& \Sexpr{m2a[3,3]}& \Sexpr{m2a[3,4]} \\\hline
			Miasto pow. 50 tys. & \Sexpr{m2a[4, 1]}& \Sexpr{m2a[4, 2]}& \Sexpr{m2a[4,3]}& \Sexpr{m2a[4,4]} \\\hline
		\end{tabular}
	\end{center}
	\caption{Tablica dwudzielcza dla kategorii \emph{Segregacja} i \emph{Miejsce zamieszkania}}
	\label{tab:tabela5}
\end{table}

	\item Tablice dla ,,wyjściowych" kategorii wiekowych (jak w przykładzie 1. z wykładu 7.):
<<>>=
m1b <- matrix(c(888,369,50,457,263,95,10,99,208,29,2,
		44,79,9,0,23),
		nrow=4,byrow=TRUE)
dimnames(m1b) <- list(c("18-25","26-35","36-45","46+"),
		c("A","B","C","D"))
@

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			& A & B & C & D \\\hline
			18\dywiz25 & \Sexpr{m1b[1, 1]}& \Sexpr{m1b[1, 2]}& \Sexpr{m1b[1, 3]}& \Sexpr{m1b[1, 4]} \\\hline
			26\dywiz35 & \Sexpr{m1b[2, 1]}& \Sexpr{m1b[2,2]}& \Sexpr{m1b[2,3]}& \Sexpr{m1b[2,4]} \\\hline
			36\dywiz45 & \Sexpr{m1b[3, 1]}& \Sexpr{m1b[3,2]}& \Sexpr{m1b[3,3]}& \Sexpr{m1b[3,4]} \\\hline
			46+ & \Sexpr{m1b[4, 1]}& \Sexpr{m1b[4, 2]}& \Sexpr{m1b[4,3]}& \Sexpr{m1b[4,4]} \\\hline
		\end{tabular}
	\end{center}
	\caption{Tablica dwudzielcza dla kategorii \emph{Segregacja} i \emph{Wiek}}
	\label{tab:tabela6}
\end{table}

<<>>=
m1c <- matrix(c(888,369,50,457,263,95,10,
		99,287,38,2,67),
		nrow=3,byrow=TRUE)
dimnames(m1c) <- list(c("18-25","26-35","36+"),
		c("A","B","C","D"))
@

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			& A & B & C & D \\\hline
			18\dywiz25 & \Sexpr{m1c[1, 1]}& \Sexpr{m1c[1, 2]}& \Sexpr{m1c[1, 3]}& \Sexpr{m1c[1, 4]} \\\hline
			26\dywiz35 & \Sexpr{m1c[2, 1]}& \Sexpr{m1c[2,2]}& \Sexpr{m1c[2,3]}& \Sexpr{m1c[2,4]} \\\hline
			36+ & \Sexpr{m1c[3, 1]}& \Sexpr{m1c[3,2]}& \Sexpr{m1c[3,3]}& \Sexpr{m1c[3,4]} \\\hline
		\end{tabular}
	\end{center}
	\caption{Tablica dwudzielcza dla kategorii \emph{Segregacja} i \emph{Wiek}}
	\label{tab:tabela7}
\end{table}
\end{itemize}
\subsubsection{Analiza niezależności}
Przeprowadziliśmy po jednym teście niezależności dla danych.\newline\noindent
Hipotezy dla testów statystycznych, w których badamy niezależność wieku, od segregacji:
\begin{itemize}[label=$\bullet$]
	\item $H_0$: \emph{Segregacja} i \emph{Wiek} są niezależne
	\item $H_1$: \emph{Segregacja} i \emph{Wiek} nie są niezależne
\end{itemize}
Hipotezy dla testu, w którym badamy niezależność segregacji i miejsca zamieszkania:
\begin{itemize}[label=$\bullet$]
	\item $H_0$: \emph{Segregacja} i \emph{Miejsce zamieszkania} są niezależne
	\item $H_1$: \emph{Segregacja} i \emph{Miejsce zamieszkania} nie są niezależne
\end{itemize}
<<warning=F>>=
fisher.test(m1a, simulate.p.value = TRUE)$p.value
chisq.test(m1a)$p.value
@


<<warning=F>>=
fisher.test(m2a, simulate.p.value = TRUE)$p.value
chisq.test(m2a)$p.value
@


<<warning=F>>=
fisher.test(m1b, simulate.p.value = TRUE)$p.value
chisq.test(m1b)$p.value
@


<<warning=F>>=
fisher.test(m1c, simulate.p.value = TRUE)$p.value
chisq.test(m1c)$p.value
@

W każdym z poniższych testów, otrzymaliśmy p{\dywiz}wartość mniejszą niż założony poziom istotności $\alpha=0.05$, więc możemy założyć, że hipoteza zerowa $H_0$ jest fałszywa i ją odrzucić, a przyjąć hipotezę alternatywną $H_1$, czyli w każdym przypadku zakładamy, że miejsce zamieszkania i segregacja śmieci nie są niezależne.

\subsubsection{Funkcja do wyliczania odpowiednich miar współzmienności zmiennych}\label{section:3.1.3}
Napisaliśmy funkcję, która jako argument przyjmuje dane oraz typ miary współzmienności, którą należy wyliczyć:
<<>>=
funkcja <- function(data, method){
	R <- nrow(data)
	C <- ncol(data)
	if (method == "GoodmanKruskalTau"){
		suma1 <- sum(sum(data^2/(sum(data) *
			rowSums(data))))
		suma2 <- sum((colSums(data)/sum(data))^2)
		(suma1 - suma2)/(1 - suma2)
	} else if (method == "CramerV") {
		sqrt((as.numeric(chisq.test(data)$statistic))
			/(sum(data) * min(R - 1, C - 1)))
	} else if (method == "T-Czuprowa"){
		sqrt((as.numeric(chisq.test(data)$statistic))
			/(sum(data) * sqrt((R-1)*(C-1))))
	} else if (method == "wspolczynnik_fi"){
		sqrt((as.numeric(chisq.test(data)$statistic))
			/sum(data))
	} else if (method == "CPearsona"){
		sqrt((as.numeric(chisq.test(data)$statistic))
		/((as.numeric(chisq.test(data)$statistic))
		+sum(data)))
	}
}
@

\subsubsection{Porównanie wyników z napisanej funkcji, z funkcjami wbudowanymi w R}

Porównaliśmy wyniki otrzymane z naszej funkcji, z funkcjami wbudowanymi w R.
\begin{itemize}
	\item Dla danych \verb|m1a|:
	\begin{itemize}[label=$\bullet$]
		\item Współczynnik Goodmana i Kruskala:
		<<warning=F>>=
		funkcja(m1a, "GoodmanKruskalTau")
		GoodmanKruskalTau(m1a, direction="column")
		@
		
		\item Współczynnik V Cramera:
		<<warning=F>>=
		funkcja(m1a, "CramerV")
		CramerV(m1a)
		@
		
		\item Współczynnik T{\dywiz}Czurpowa
		<<warning=F>>=
		funkcja(m1a, "T-Czuprowa")
		TschuprowT(m1a)
		@
		
		\item Współczynnik $\varphi$:
		<<warning=F>>=
		funkcja(m1a, "wspolczynnik_fi")
		Phi(m1a)
		@
		
		\item Współczynnik C Pearsona:
		<<warning=F>>=
		funkcja(m1a, "CPearsona")
		ContCoef(m1a)
		@
	\end{itemize}

	\item Dla danych \verb|m2a|:
	\begin{itemize}[label=$\bullet$]
		\item Współczynnik Goodmana i Kruskala:
		<<warning=F>>=
		funkcja(m2a, "GoodmanKruskalTau")
		GoodmanKruskalTau(m2a, direction="column")
		@
		
		\item Współczynnik V Cramera:
		<<warning=F>>=
		funkcja(m2a, "CramerV")
		CramerV(m2a)
		@
		
		\item Współczynnik T{\dywiz}Czurpowa
		<<warning=F>>=
		funkcja(m2a, "T-Czuprowa")
		TschuprowT(m2a)
		@
		
		\item Współczynnik $\varphi$:
		<<warning=F>>=
		funkcja(m2a, "wspolczynnik_fi")
		Phi(m2a)
		@
		
		\item Współczynnik C Pearsona:
		<<warning=F>>=
		funkcja(m2a, "CPearsona")
		ContCoef(m2a)
		@
	\end{itemize}

	\item Dla danych \verb|m1b|:
	\begin{itemize}[label=$\bullet$]
		\item Współczynnik Goodmana i Kruskala:
		<<warning=F>>=
		funkcja(m1b, "GoodmanKruskalTau")
		GoodmanKruskalTau(m1b, direction="column")
		@
		
		\item Współczynnik V Cramera:
		<<warning=F>>=
		funkcja(m1b, "CramerV")
		CramerV(m1b)
		@
		
		\item Współczynnik T{\dywiz}Czurpowa
		<<warning=F>>=
		funkcja(m1b, "T-Czuprowa")
		TschuprowT(m1b)
		@
		
		\item Współczynnik $\varphi$:
		<<warning=F>>=
		funkcja(m1b, "wspolczynnik_fi")
		Phi(m1b)
		@
		
		\item Współczynnik C Pearsona:
		<<warning=F>>=
		funkcja(m1b, "CPearsona")
		ContCoef(m1b)
		@
	\end{itemize}

	\item Dla danych \verb|m1c|:
	\begin{itemize}[label=$\bullet$]
		\item Współczynnik Goodmana i Kruskala:
		<<warning=F>>=
		funkcja(m1c, "GoodmanKruskalTau")
		GoodmanKruskalTau(m1c, direction="column")
		@
		
		\item Współczynnik V Cramera:
		<<warning=F>>=
		funkcja(m1c, "CramerV")
		CramerV(m1c)
		@
		
		\item Współczynnik T{\dywiz}Czurpowa
		<<warning=F>>=
		funkcja(m1c, "T-Czuprowa")
		TschuprowT(m1c)
		@
		
		\item Współczynnik $\varphi$:
		<<warning=F>>=
		funkcja(m1c, "wspolczynnik_fi")
		Phi(m1c)
		@
		
		\item Współczynnik C Pearsona:
		<<warning=F>>=
		funkcja(m1c, "CPearsona")
		ContCoef(m1c)
		@
	\end{itemize}
\end{itemize}

\subsubsection{Analiza Korespondencji}\label{section:3.1.5}
Poniżej przedstawiliśmy macierze, które uzyskaliśmy z naszej funkcji, w celu przeprowadzenia analizy korespondencji. Wartości w macierzy zaokrągliliśmy do czwartego miejsca po przecinku: 
\begin{itemize}[label=$\bullet$]
	\item Dla danych \verb|m1a|:
	\begin{align}
		\text{\textbf{P:}}
%%%%%%%%%%%%%%%%%%%%%%
\begin{bmatrix*}[r]
	\Sexpr{round(0.3382857143, 4)} & \Sexpr{round(0.140571429, 4)} & \Sexpr{round(0.0190476190, 4)} & \Sexpr{round(0.174095238, 4)} \\
	\Sexpr{round(0.1001904762, 4)} & \Sexpr{round(0.036190476, 4)} & \Sexpr{round(0.0038095238, 4)} & \Sexpr{round(0.037714286, 4)} \\
	\Sexpr{round(0.0792380952, 4)} & \Sexpr{round(0.011047619, 4)} & \Sexpr{round(0.0007619048, 4)} & \Sexpr{round(0.016761905, 4)} \\
	\Sexpr{round(0.0297142857, 4)} & \Sexpr{round(0.003428571, 4)} & \Sexpr{round(0.0000000000, 4)} & \Sexpr{round(0.007238095, 4)} \\
	\Sexpr{round(0.0003809524, 4)} & \Sexpr{round(0.000000000, 4)} & \Sexpr{round(0.0000000000, 4)} & \Sexpr{round(0.001523810, 4)}
\end{bmatrix*}, &
\text{\textbf{A:}}
\begin{bmatrix*}[r]
	\Sexpr{round(-0.049184998, 4)} & \Sexpr{round(0.03363991, 4)} & \Sexpr{round(0.025206501, 4)} & \Sexpr{round(0.03657666, 4)} \\
	\Sexpr{round(0.008753063, 4)} & \Sexpr{round(0.01175547, 4)} & \Sexpr{round(-0.006053731, 4)} & \Sexpr{round(-0.02194086, 4)} \\
	\Sexpr{round(0.083034110, 4)} & \Sexpr{round(-0.06664708, 4)} & \Sexpr{round(-0.035362706, 4)} & \Sexpr{round(-0.05516979, 4)} \\
	\Sexpr{round(0.051053163, 4)} & \Sexpr{round(-0.04886150, 4)} & \Sexpr{round(-0.030882999, 4)} & \Sexpr{round(-0.02396049, 4)} \\
	\Sexpr{round(-0.020509120, 4)} & \Sexpr{round(-0.01908568, 4)} & \Sexpr{round(-0.006707359, 4)} &  \Sexpr{round(0.05040715, 4)}
\end{bmatrix*}\\
\text{\textbf{R:}}
\begin{bmatrix*}[r]
  \Sexpr{round(0.672000000, 4)} & \Sexpr{round(0.177904762, 4)} & \Sexpr{round(0.107809524, 4)} & \Sexpr{round(0.040380952, 4)} & \Sexpr{round(0.001904762, 4)}
\end{bmatrix*}, & \text{\textbf{C:}} 
\begin{bmatrix*}[r]
\Sexpr{round(0.54780952, 4)} & \Sexpr{round(0.19123810, 4)} & \Sexpr{round(0.02361905, 4)} & \Sexpr{round(0.23733333, 4)}
\end{bmatrix*}\\
    \text{\textbf{F:}} 
    \begin{bmatrix*}[r]
		 \Sexpr{round(-0.09048091, 4)} &   \Sexpr{round(0.003935358, 4)} &  \Sexpr{round(-0.00424245, 4)} &  \Sexpr{round(-7.972808e-17, 4)} \\
		 \Sexpr{round(0.02838125, 4)} &  \Sexpr{round(-0.054172767, 4)} &   \Sexpr{round(0.01947277, 4)} &  \Sexpr{round(-9.588423e-17, 4)} \\
		 \Sexpr{round(0.38035147, 4)} &   \Sexpr{round(0.012314854, 4)} &  \Sexpr{round(-0.01287347, 4)} &  \Sexpr{round(-1.057415e-16, 4)} \\
		 \Sexpr{round(0.39268765, 4)} &   \Sexpr{round(0.083974027, 4)} &   \Sexpr{round(0.01581285, 4)} &  \Sexpr{round(-1.752284e-17, 4)} \\
		 \Sexpr{round(-0.58201684, 4)} &   \Sexpr{round(1.194071935, 4)} &  \Sexpr{round(0.07138559, 4)} &  \Sexpr{round(-2.257451e-16, 4)}
	\end{bmatrix*}, & 
    \text{\textbf{G:}}
  \begin{bmatrix*}[r]
		\Sexpr{round(0.1504620, 4)} & \Sexpr{round(-0.005221131, 4)} & \Sexpr{round(-0.001210770, 4)} & \Sexpr{round(8.498321e-17, 4)} \\
		\Sexpr{round(-0.1915971, 4)} & \Sexpr{round(-0.086311189, 4)} &  \Sexpr{round(0.009831168, 4)} & \Sexpr{round(8.498321e-17, 4)} \\
		\Sexpr{round(-0.3347124, 4)}  & \Sexpr{round(-0.086721500, 4)} & \Sexpr{round(-0.064268200, 4)} & \Sexpr{round(8.498321e-17, 4)} \\
		\Sexpr{round(-0.1595994, 4)} & \Sexpr{round(0.090229432, 4)} & \Sexpr{round( 0.001268811, 4)} & \Sexpr{round(8.498321e-17, 4)}
\end{bmatrix*}
\end{align}
  <<analiza_koresp_m1a, echo=F, warning=F, fig.cap="Porównanie wykresu analizy korespondencji z naszej funkcji (na górze) z wykresem funkcji wbudowanej (na dole), dla danych m1a">>=
  par(mfrow=c(2, 1))
	  analiza.kores(m1a, columnsName=c('A', 'B', 'C', 'D'), 
	                rowsName=c('18-25', '26-35', '36-45', '46-59', '60+'))
	  plot(ca(m1a))
	  
	@
  \item Dla danych \verb|m2a|:
  \begin{align}
		\text{\textbf{P:}}
		\begin{bmatrix*}[r]
			\Sexpr{round(0.19245427, 4)} & \Sexpr{round(0.07698171, 4)} & \Sexpr{round(0.007240854, 4)} & \Sexpr{round(0.05182927, 4)} \\
			\Sexpr{round(0.09146341, 4)} & \Sexpr{round(0.02934451, 4)} & \Sexpr{round(0.005335366, 4)} & \Sexpr{round(0.03353659, 4)} \\
			\Sexpr{round(0.06897866, 4)} & \Sexpr{round(0.02400915, 4)} & \Sexpr{round(0.003048780, 4)} & \Sexpr{round(0.04001524, 4)} \\
			\Sexpr{round(0.19512195, 4)} & \Sexpr{round(0.06059451, 4)} & \Sexpr{round(0.008003049, 4)} & \Sexpr{round(0.11204268, 4)}
		\end{bmatrix*}, & 
  	\text{\textbf{A:}}
  	\begin{bmatrix*}[r]
		\Sexpr{round(0.02928830, 4)} &  \Sexpr{round(0.056939447, 4)} & \Sexpr{round(-0.005914787, 4)} & \Sexpr{round(-0.09369183, 4)} \\
		\Sexpr{round(0.01337288, 4)} & \Sexpr{round(-0.006546968, 4)} &  \Sexpr{round(0.025436939, 4)} & \Sexpr{round(-0.02247047, 4)} \\
		\Sexpr{round(-0.02043631, 4)} & \Sexpr{round(-0.012205686, 4)} & \Sexpr{round(-0.002925313, 4)} &  \Sexpr{round(0.04291668, 4)} \\
		\Sexpr{round(-0.02380534, 4)} & \Sexpr{round(-0.041626547, 4)} & \Sexpr{round(-0.009291271, 4)} &  \Sexpr{round(0.07642676, 4)}
	\end{bmatrix*} \\
  	\text{\textbf{R:}}
  	\begin{bmatrix*}[r]
    \Sexpr{round(0.3285061, 4)}& \Sexpr{round(0.1596799, 4)}& \Sexpr{round(0.1360518, 4)}& \Sexpr{round(0.3757622, 4)}
    \end{bmatrix*}, & 
		\text{\textbf{C:}}
		\begin{bmatrix*}[r]
    \Sexpr{round(0.54801829, 4)} & \Sexpr{round(0.19092988, 4)} & \Sexpr{round(0.02362805, 4)} & \Sexpr{round(0.23742378, 4)}
    \end{bmatrix*} \\
		\text{\textbf{F:}}
		\begin{bmatrix*}[r]
			\Sexpr{round(-0.19713141, 4)} &  \Sexpr{round(0.021106770, 4)} &  \Sexpr{round(0.001452104, 4)} & \Sexpr{round(9.742502e-17, 4)} \\
			\Sexpr{round(-0.05195857, 4)}& \Sexpr{round(-0.076757025, 4)} & \Sexpr{round(-0.003351379, 4)} & \Sexpr{round(9.742502e-17, 4)} \\
			\Sexpr{round(0.12933039, 4)} &  \Sexpr{round(0.022376636, 4)} & \Sexpr{round(-0.023203022, 4)} & \Sexpr{round(9.742502e-17, 4)} \\
			\Sexpr{round(0.14759328, 4)} &  \Sexpr{round(0.006063589, 4)} &  \Sexpr{round(0.008555774, 4)} & \Sexpr{round(9.742502e-17, 4)}
		\end{bmatrix*}, &
		\text{\textbf{G:}}
		\begin{bmatrix*}[r]
			\Sexpr{round(-0.05897788, 4)} & \Sexpr{round(-0.01236668, 4)} &  \Sexpr{round(0.007514201, 4)} & \Sexpr{round(9.742502e-17, 4)} \\
			\Sexpr{round(-0.15839872, 4)} & \Sexpr{round( 0.04244696, 4)} & \Sexpr{round(-0.012995446, 4)} & \Sexpr{round(9.742502e-17, 4)} \\
			\Sexpr{round(-0.03552871, 4)} & \Sexpr{round(-0.17322761, 4)} & \Sexpr{round(-0.040132853, 4)} & \Sexpr{round(9.742502e-17, 4)} \\
			\Sexpr{round(0.26704773, 4)} &  \Sexpr{round(0.01164924, 4)} & \Sexpr{round(-0.002899623, 4)} & \Sexpr{round(9.742502e-17, 4)}
                \end{bmatrix*}
    \end{align}
    <<analiza_koresp_m2a, echo=F, warning=F, fig.cap="Porównanie wykresu analizy korespondencji z naszej funkcji (na górze) z wykresem funkcji wbudowanej (na dole), dla danych m2a">>=
    par(mfrow=c(2, 1))
  	  analiza.kores(m2a, columnsName=c('A', 'B', 'C', 'D'), 
  	                rowsName=c('Wieś', 'Miasto do 20tys', 'Miasto 20-50tys', 'Miasto 50tys+'))
  	  plot(ca(m2a))
  	@
  \item Dla danych \verb|m1b|:
    \begin{align}
		  \text{\textbf{P:}}
		  \begin{bmatrix*}[r]
			\Sexpr{round(0.33828571, 4)} & \Sexpr{round(0.140571429, 4)} & \Sexpr{round(0.0190476190, 4)}& \Sexpr{round(0.174095238, 4)} \\
			\Sexpr{round(0.10019048, 4)} & \Sexpr{round(0.036190476, 4)} & \Sexpr{round(0.0038095238, 4)} & \Sexpr{round(0.037714286, 4)} \\
			\Sexpr{round(0.07923810, 4)} & \Sexpr{round(0.011047619, 4)} & \Sexpr{round(0.0007619048, 4)} & \Sexpr{round(0.016761905, 4)} \\
			\Sexpr{round(0.03009524, 4)} & \Sexpr{round(0.003428571, 4)} & \Sexpr{round(0.0000000000, 4)} & \Sexpr{round(0.008761905, 4)}
		\end{bmatrix*}, &
      \text{\textbf{A:}}
      \begin{bmatrix*}[r]
			\Sexpr{round(-0.049184998, 4)} & \Sexpr{round(0.03363991, 4)} &  \Sexpr{round(0.025206501, 4)} & \Sexpr{round(0.03657666, 4)} \\
			\Sexpr{round(0.008753063, 4)} &  \Sexpr{round(0.01175547, 4)} & \Sexpr{round(-0.006053731, 4)} & \Sexpr{round(-0.02194086, 4)} \\
			\Sexpr{round(0.083034110, 4)} & \Sexpr{round(-0.06664708, 4)} & \Sexpr{round(-0.035362706, 4)} & \Sexpr{round(-0.05516979, 4)} \\
			\Sexpr{round(0.045537250, 4)} & \Sexpr{round(-0.05179905, 4)} & \Sexpr{round(-0.031602979, 4)} & \Sexpr{round(-0.01271630, 4)}
		\end{bmatrix*} \\
      \text{\textbf{R:}}
      \begin{bmatrix*}[r]
\Sexpr{round(0.67200000, 4)} & \Sexpr{round(0.17790476, 4)} & \Sexpr{round(0.10780952, 4)} & \Sexpr{round(0.04228571, 4)}
\end{bmatrix*}, &
      \text{\textbf{C:}}
      \begin{bmatrix*}[r]
\Sexpr{round(0.54780952, 4)} & \Sexpr{round(0.19123810, 4)} & \Sexpr{round(0.02361905, 4)} & \Sexpr{round(0.23733333, 4)}
\end{bmatrix*} \\
      \text{\textbf{F:}}
      \begin{bmatrix*}[r]
			\Sexpr{round(-0.08991485, 4)} & \Sexpr{round(0.01133184, 4)} & \Sexpr{round(-0.002681055, 4)} & \Sexpr{round(1.054335e-16, 4)} \\
			\Sexpr{round(0.02414904, 4)} & \Sexpr{round(-0.05803904, 4)} &  \Sexpr{round(0.012948596, 4)} & \Sexpr{round(1.054335e-16, 4)} \\
			\Sexpr{round(0.38011175, 4)} & \Sexpr{round(-0.01559436, 4)} & \Sexpr{round(-0.016015489, 4)} & \Sexpr{round(1.054335e-16, 4)} \\
			\Sexpr{round(0.35820337, 4)} &  \Sexpr{round(0.10385648, 4)} &  \Sexpr{round(0.028961892, 4)} & \Sexpr{round(1.054335e-16, 4)}
		\end{bmatrix*}, & 
      \text{\textbf{G:}}
      \begin{bmatrix*}[r]
			\Sexpr{round(0.1445734, 4)} & \Sexpr{round(-0.004895096, 4)} & \Sexpr{round(-0.001303018, 4)} & \Sexpr{round(-1.054335e-16, 4)} \\
			\Sexpr{round(-0.2034402, 4)} & \Sexpr{round(-0.049534317, 4)} &  \Sexpr{round(0.007456756, 4)} & \Sexpr{round(-1.054335e-16, 4)} \\
			\Sexpr{round(-0.3459323, 4)} & \Sexpr{round(-0.021232231, 4)} & \Sexpr{round(-0.059694817, 4)} & \Sexpr{round(-1.054335e-16, 4)} \\
			\Sexpr{round(-0.1353480, 4)} &  \Sexpr{round(0.053325479, 4)} &  \Sexpr{round(0.002939852, 4)} & \Sexpr{round(-1.054335e-16, 4)}
		\end{bmatrix*}
    \end{align}
    <<analiza_koresp_m1b,echo=F, warning=F, fig.cap="Porównanie wykresu analizy korespondencji z naszej funkcji (na górze) z wykresem funkcji wbudowanej (na dole), dla danych m1b">>=
    par(mfrow=c(2, 1))
  	  analiza.kores(m1b, columnsName=c('A', 'B', 'C', 'D'), 
  	                rowsName=c('18-25', '26-35', '36-45', '46+'))
  	  plot(ca(m1b))
  	@
    \item Dla danych \verb|m1c|:
    \begin{align}
		  \text{\textbf{P:}}
		  \begin{bmatrix*}[r]
				\Sexpr{round(0.3382857, 4)} & \Sexpr{round(0.14057143, 4)} & \Sexpr{round(0.0190476190, 4)} & \Sexpr{round(0.17409524, 4)} \\
				\Sexpr{round(0.1001905, 4)} & \Sexpr{round(0.03619048, 4)} & \Sexpr{round(0.0038095238, 4)} & \Sexpr{round(0.03771429, 4)} \\
				\Sexpr{round(0.1093333, 4)} & \Sexpr{round(0.01447619, 4)} & \Sexpr{round(0.0007619048, 4)} & \Sexpr{round(0.02552381, 4)}
		\end{bmatrix*}, &
      \text{\textbf{A:}}
      \begin{bmatrix*}[r]
			\Sexpr{round(-0.049184998, 4)} &  \Sexpr{round(0.03363991 , 4)}&  \Sexpr{round(0.025206501, 4)} &  \Sexpr{round(0.03657666, 4)} \\
			\Sexpr{round(0.008753063, 4)} &  \Sexpr{round(0.01175547, 4)} & \Sexpr{round(-0.006053731, 4)} & \Sexpr{round(-0.02194086, 4)} \\
			\Sexpr{round(0.094542479, 4)} & \Sexpr{round(-0.08397793, 4)} & \Sexpr{round(-0.046744450, 4)} & \Sexpr{round(-0.05350652, 4)}
			\end{bmatrix*} \\
      \text{\textbf{R:}}
      \begin{bmatrix*}[r]
      \Sexpr{round(0.6720000, 4)} & \Sexpr{round(0.1779048, 4)} & \Sexpr{round(0.1500952, 4)}
      \end{bmatrix*}, &
      \text{\textbf{C:}}
      \begin{bmatrix*}[r]
      \Sexpr{round(0.54780952, 4)} & \Sexpr{round(0.19123810, 4)} & \Sexpr{round(0.02361905, 4)} & \Sexpr{round(0.23733333, 4)}
      \end{bmatrix*} \\
      \text{\textbf{F:}}
      \begin{bmatrix*}[r]
		\Sexpr{round(0.08988251, 4)} &  \Sexpr{round(0.01189178, 4)} & \Sexpr{round(7.677186e-17, 4)} \\
			\Sexpr{round(-0.02398484, 4)} & \Sexpr{round(-0.05953235, 4)} & \Sexpr{round(7.677186e-17, 4)} \\
			\Sexpr{round(-0.37398941, 4)} &  \Sexpr{round(0.01732107, 4)} & \Sexpr{round(7.677186e-17, 4)}
	\end{bmatrix*}, &
      \text{\textbf{G:}}
      \begin{bmatrix*}[r]
				\Sexpr{round(-0.1444381, 4)} & \Sexpr{round(-0.003155414, 4)} & \Sexpr{round(-7.313518e-17, 4)} \\
				\Sexpr{round(0.2039737, 4)} & \Sexpr{round(-0.043724362, 4)} & \Sexpr{round(-8.147830e-17, 4)} \\
				\Sexpr{round(0.3472406, 4)} &  \Sexpr{round(0.019701549, 4)} & \Sexpr{round(1.048313e-17, 4)} \\
				\Sexpr{round(0.1344756, 4)}&  \Sexpr{round(0.040554765, 4)} & \Sexpr{round(-8.442593e-17, 4)}
			\end{bmatrix*}
  \end{align}
  <<analiza_koresp_m1c, echo=F, warning=F, fig.cap="Porównanie wykresu analizy korespondencji z naszej funkcji (na górze) z wykresem funkcji wbudowanej (na dole), dla danych m1c">>=
  par(mfrow=c(2, 1))
	  analiza.kores(m1c, columnsName=c('A', 'B', 'C', 'D'), 
	                rowsName=c('18-25', '26-35', '36+'))
	  plot(ca(m1c))
	@
\end{itemize}
Wyznaczyliśmy jeszcze inercję całkowitą dla wszytskich danych, korzystając ze wzoru:
\begin{equation}\label{eq:inercja}
	\lambda=\left(\text{tr}\mathbf{A}^T\mathbf{A}\right)
\end{equation}
gdzie macierz $\mathbf{A}$, to macierz uzyskana podczas analizy korespondencji i otrzymaliśmy następujące wartości:
\begin{itemize}[label=$\bullet$]
	\item Dla danych \verb|m1a|:
	<<echo=F>>=
	  m1aA <- matrix(c(-0.049184998,  0.03363991,  0.025206501,  0.03657666,
	0.008753063,  0.01175547, -0.006053731, -0.02194086,
	0.083034110, -0.06664708, -0.035362706, -0.05516979,
	0.051053163, -0.04886150, -0.030882999, -0.02396049,
	-0.020509120, -0.01908568, -0.006707359,  0.05040715), 
	nrow=5, byrow = TRUE)
	@
	<<>>=
	m1aA
	sum(diag(m1aA %*% t(m1aA)))
	@
	\item Dla danych \verb|m2a|:
	<<echo=F>>=
	    m2aA <- matrix(c(0.02928830,  0.056939447, -0.005914787, -0.09369183,
	0.01337288, -0.006546968,  0.025436939, -0.02247047,
	-0.02043631, -0.012205686, -0.002925313,  0.04291668,
	-0.02380534, -0.041626547, -0.009291271,  0.07642676),
	nrow=4,byrow=TRUE)
	@
	<<>>=
	m2aA
	sum(diag(m2aA %*% t(m2aA)))
	@
	\item Dla danych \verb|m1b|:
	<<echo=F>>=
	m1bA <- matrix(c(-0.049184998,  0.03363991,  0.025206501,  0.03657666,
	0.008753063,  0.01175547, -0.006053731, -0.02194086,
	0.083034110, -0.06664708, -0.035362706, -0.05516979,
	0.045537250, -0.05179905, -0.031602979, -0.01271630), 
	nrow=4, byrow = TRUE)
	@
	<<>>=
	m1bA
	sum(diag(m1bA %*% t(m1bA)))
	@
	\item Dla danych \verb|m1c|:
	<<echo=F>>=
	m1cA <- matrix(c(-0.049184998,  0.03363991,  0.025206501,  0.03657666,
	0.008753063,  0.01175547, -0.006053731, -0.02194086,
	0.094542479, -0.08397793, -0.046744450, -0.05350652), 
	nrow=3, byrow = TRUE)
	@
	<<>>=
	m1cA
	sum(diag(m1cA %*% t(m1cA)))
	@
\end{itemize}
\subsubsection{Wnioski}
Analizę rozpoczęliśmy od przeprowadzenia testów niezależności Fishera i chi{\dywiz}kwadrat, na poziomie istotności $\alpha=0.01$. Test Fishera dla każdej z tabel dwudzielczych ((\ref{tab:tabela4}), (\ref{tab:tabela5}), (\ref{tab:tabela6}), (\ref{tab:tabela7})) wyznaczył taką samą p{\dywiz}wartość równą w przybliżeniu 0.0005. Przez to, że wszystkie p{\dywiz}wartości są dokładnie takie same, możemy sądzić, że nie są spełnione założenia do przeprowadzenia testu Fishera, jak na przykład to, że w niektórych pozycjach tabeli występują zera, lub bardzo małe wartości. P{\dywiz}wartości uzyskane w teście chi{\dywiz}kwadrat są praktycznie równe zero, więc mamy podstawy do odrzucenia hipotezy o niezależności $H_0$, czyli zakładamy dla wszystkich zmiennych, że nie są niezależne.

Skoro odrzuciliśmy hipotezy o niezależności dla wszystkich rozważanych tabel dwudzielczych, to możemy teraz badać współzmienność dla zmiennych nominalnych. Współczynniki zmienności wyznaczyliśmy na podstawie naszej funkcji i funkcji wbudowanej w pakiecie R. Dla wszystkich przypadków uzyskaliśmy dokładnie takie same wartości dla funkcji wbudowanych i naszej funkcji, więc możemy stwierdzić, że nasza funkcja została zaimplementowana poprawnie.

Następnie przeszliśmy do analizy korespondencji, dzięki której możemy odczytać jakie zachodzą relacje między rozważanymi zmiennymi. Wykresy napisanej przez nas funkcji i wykresy funkcji wbudowanej w pakiecie R są takie same, więc możemy stwierdzić, że nasza funkcja do analizy korespondencji została zaimplementowana poprawnie. Na sam koniec wyliczyliśmy inercję całkowitą. Inercję całkowitą możemy interpretować jako miarę rozproszenia profili w przestrzeni wielowymiarowej (im jest większa, tym punkty bardziej są rozproszone).

Poniżej przedstawiliśmy wnioski jakie możemy wyciągnąć dla każdych danych, na podstawie wyznaczonych współczynników zmienności i analizy korespondencji:
\begin{itemize}[label=$\bullet$]
	\item Dla danych \verb|m1a|:\newline
		Wszystkie wyznaczone miary współzmienności mieszczą się w przedziale $[0.09, 0.18]$, przez co możemy stwierdzić, że nasze zmienne charakretyzują się słabą współzmiennością.\newline\noindent
		Na rysunku (\ref{fig:analiza_koresp_m1a}) przedstawiliśmy dwa wykresy analizy korespondencji. Pierwsza zmienna, która się wyróżnia na wykresie, to zmienna opisująca osoby 60+, które brały udział w ankiecie. W ankiecie brało tylko 5 osób 60+, więc trudno wyciągać wnioski dla tej klasyfikacji. Oprócz tego możemy stwierdzić, że stosunkowo najczęstszą odpowiedzią jakie udzielały osoby w wieku 46 \ppauza 59, 36 \ppauza 45 i 26 \ppauza 35 to odpowiedź A (\emph{Segreguję śmieci, ponieważ jest to korzystne dla środowiska}), ponieważ punkty odpowiadające ich kategoriom, znajdują się najbliżej tej odpowiedzi. Odpowiedź D była najczęściej udzielana przez osoby w wieku 18 \ppauza 25 lat. Odpowiedź C (\emph{Segreguję śmieci, ponieważ wszyscy tak robią}) była najrzadziej wybieraną spośród wszystkich, ponieważ punkt reprezentujący tę odpowiedź znajduje się najdalej od wszystkich  kategorii reprezentujących wiek osób ankietowanych;\newline\noindent
		Procenty wyjaśnienia dla dwóch osi współrzędnych sumują się prawie do 100\%, więc relacja jest dość dobrze przedstawiona.
		
		Obliczając inercję całkowitą uzyskaliśmy wartość w przybliżeniu równą 0.032.
	\item Dla danych \verb|m2a|:\newline
		Wszystkie wyznaczone miary współzmienności, poza współczynnikiem Goodmana i Kruskala mieszczą się przedziale $[0.09, 0.16]$, natomiast współczynnik Goodmana i Kruskala wyszedł w przybliżeniu równy 0.01. Korzystając z wykładu, wiemy, że współczynnik Goodmana i Kruskala jest najlepszy spośród rozważanych. Na tej podstawie możemy wyciągnąć wniosek, że nasze zmienne charakteryzują się bardzo słabą współzmiennością. \newline\noindent
		Na rysunku (\ref{fig:analiza_koresp_m2a}) przedstawiliśmy wykresy analizy korespondencji. Procenty objaśnienia sumjują się prawie do 100\%, więc relacja między zmiennymi jest dobrze przedstawiona. Na podstawie wykresu możemy powiedzieć, że stosunkowo, wśród ankietowanych osób, te które nie segregują śmieci mieszkają najczęściej w większych miastach (miasta powyżej 20 tys mieszkańców), ponieważ punkt reprezentujący odpowiedź D, znajduje się najbliżej punktów reprezentujących te miasta. Ludzie segregujący śmieci ze względu na taki wymóg ustawowy, stosunkowo najczęściej mieszkają na wsi. Najrzadziej wybieraną odpowiedzią, była odpowiedź C, ponieważ znajduje się na wykresie stosunkowo najdalej od innych odpowiedzi.
		
		Obliczając inercję całkowitą uzyskaliśmy wartość w przybliżeniu równą 0.025. Jest mniejsza niż w przypadku danych \verb|m1a|, co zgadza się z intuicją, bo porównując rysunki ((\ref{fig:analiza_koresp_m1a}), (\ref{fig:analiza_koresp_m2a})) widać, że dla danych \verb|m2a| nie ma wartości tak odstającej jak dla zmiennej \verb|m1a|.
	\item Dla danych \verb|m1b| oraz \verb|m1c|:\newline
		Podsumowanie dla zmiennych \verb|m1b| i \verb|m1c| przeprowadziliśmy wspólnie, ponieważ na podstawie uzyskanych wyników możemy wyciągnąć podobne wnioski.
		
		Zmienne \verb|m1b| i \verb|m1c| różnią tym, że w zmiennej \verb|m1c| połączone zostały kategorie wiekowe 36 \ppauza 45 i 46+ z danych \verb|m1b| (tabele: (\ref{tab:tabela6}) (\ref{tab:tabela7})).\newline\noindent
		Uzyskane współczynniki zmienności mieszczą się w przedziale [0.09, 0.17] przez co możemy wnioskować, że zmienne charakteryzują się słabą współzmiennością.\newline\noindent
		Na rysunkach (\ref{fig:analiza_koresp_m1b}) i (\ref{fig:analiza_koresp_m1c}) przedstawiliśmy wykresy analizy korespondencji. Pierwszą rzeczą na którą warto zwrócić uwagę jest to, że połączenie kategorii 60+ z zmiennej \verb|m1a| do kategorii 46 \ppauza 59 spowodowało, że nie widać teraz punktu, który jest znacznie oddalony od innych. Pozostałe wnioski możemy wyciągnąć podobnie jak dla zmiennej \verb|m1a|. Odpowiedź D była udzielana stosunkowo najczęściej przez najmłodszych ankietowanych, osoby ankietowane powyżej 25 roku życia stosunkowo najczęściej segregują śmieci, ze względu na ochronę środowiska. Odpowiedź C (\emph{Segreguję śmieci, bo inni tak robią}) była najrzadziej wybierana.\newline\noindent
		Suma procentów zmienności daje wartość równą 100\%, więc relacje między kategoriami są dobrze przedstawione.
		
		Inercja całkowita dla rozważanych danych wyszła bardzo podobna, odpowiednio dla danych \verb|m1b| i \verb|m1c| \pauza 0.028 i 0.027.
\end{itemize}
\subsection{Zadanie 2}\label{section:3.2}
W tym zadaniu należało wyznaczyć odpowiednie miary współzmienności i przeprowadzić analizę korespondencji (podobnie jak w zadaniu 1. z tej listy\pauza(\ref{section:3.1}))
\subsubsection{Dane do analizy}
W tym zadaniu analizujemy dane, które już się pojawiły w zadaniu 2. w liście 5\_6\_7 \pauza
(\ref{section:2.1}), (tabela (\ref{tab:tabela1}))
<<>>=
data <- matrix(c(35, 0, 0,
		22, 22, 0,
		15, 15, 15,
		0, 40, 10,
		18, 3, 5), byrow=T, nrow=5)
dimnames(data) <- list(c('Ibuprom', 'Apap', 'Paracetamol', 'Ibuprofen', 'Panadol'),
		c('do lat 35', 'od 36 do 55', 'powyżej 55'))
data
@
\subsubsection{Analiza niezależności}
Przeprowadziliśmy dwa testy niezależności (\verb|chisq.test| i \verb|fisher.test|) dla danych.\newline\noindent
Hipotezy dla testów:
\begin{itemize}[label=$\bullet$]
	\item $H_0$: \emph{Wiek} i \emph{Lek} są niezależne
	\item $H_1$:\emph{Wiek} i \emph{Lek} nie są niezależne
\end{itemize}
<<>>=
fisher.test(data, simulate.p.value=T)$p.value
@

<<warning=F>>=
chisq.test(data)$p.value
@

Wartość p obydwu testów jest znacznie niższa od założonego poziomu istotności $\alpha=0.05$ więc możemy założyć, że hipoteza zerowa $H_0$ jest fałszywa \pauza \emph{Wiek} i \emph{Lek} nie są niezależne.
\subsubsection{Miary współzmienności}
W tej części pracy wyliczyliśmy miary współzmienności dla rozważanych danych, korzystając z funkcji napisanej w podrozdziale (\ref{section:3.1.3}) i porównaliśmy wyniki z funkcjami wbudowanymi.

\begin{itemize}[label=$\bullet$]
	\item Współczynnik Goodmana i Kruskala:
	<<warning=F>>=
	funkcja(data, "GoodmanKruskalTau")
	GoodmanKruskalTau(data, direction="column")
	@
	
	\item Współczynnik V Cramera:
	<<warning=F>>=
	funkcja(data, "CramerV")
	CramerV(data)
	@
	
	\item Współczynnik T{\dywiz}Czurpowa
	<<warning=F>>=
	funkcja(data, "T-Czuprowa")
	TschuprowT(data)
	@
	
	\item Współczynnik $\varphi$:
	<<warning=F>>=
	funkcja(data, "wspolczynnik_fi")
	Phi(data)
	@
	
	\item Współczynnik C Pearsona:
	<<warning=F>>=
	funkcja(data, "CPearsona")
	ContCoef(data)
	@
\end{itemize}

\subsubsection{Analiza korespondencji}
Poniżej przedstawiliśmy macierze, które uzyskaliśmy z naszej funkcji, w celu przeprowadzenia analizy korespondencji. Wartości w macierzy zaokrągliliśmy do czwartego miejsca po przecinku:
\begin{align}
	  \text{\textbf{P:}}
	  \begin{bmatrix*}[r]
			0.175 & 0.000 & 0.000 \\
			0.110 & 0.110 & 0.000 \\
			0.075 & 0.075 & 0.075 \\
			0.000 & 0.200 & 0.050 \\
			0.090 & 0.015 & 0.025
	\end{bmatrix*}, &
    \text{\textbf{A:}}
    \begin{bmatrix*}[r]
			\Sexpr{round(0.34298526, 4)} & \Sexpr{round(-0.26457513, 4)} & \Sexpr{round(-0.16201852, 4)} \\
			\Sexpr{round(0.03496029, 4)} &  \Sexpr{round(0.07416198, 4)} & \Sexpr{round(-0.18165902, 4)} \\
			\Sexpr{round(-0.08249579, 4)} & \Sexpr{round(-0.05000000, 4)} & \Sexpr{round( 0.22453656, 4)} \\
			\Sexpr{round(-0.33541020, 4)} & \Sexpr{round(0.31622777, 4)} &  \Sexpr{round(0.06454972, 4)} \\
			\Sexpr{round(0.13023647, 4)} & \Sexpr{round(-0.16225573, 4)} &  \Sexpr{round(0.03938632, 4)}
	\end{bmatrix*} \\
    \text{\textbf{R:}}
    \begin{bmatrix*}[r]
0.175 & 0.220 & 0.225 & 0.250 & 0.130
\end{bmatrix*}, &
    \text{\textbf{C:}}
\begin{bmatrix*}[r]
0.45 & 0.40 & 0.15
\end{bmatrix*} \\
    \text{\textbf{F:}}
    \begin{bmatrix*}[r]
			\Sexpr{round(-1.09956877, 4)} &  \Sexpr{round(0.1147638, 4)} & \Sexpr{round(-8.777254e-17, 4)} \\
			\Sexpr{round(-0.05710257, 4)} & \Sexpr{round(0.4210640, 4)} & \Sexpr{round(-1.305046e-16, 4)} \\
			\Sexpr{round(0.18562161, 4)} & \Sexpr{round(-0.4806004, 4)} & \Sexpr{round(-1.202804e-16, 4)} \\
			\Sexpr{round(0.92250490, 4)} &  \Sexpr{round(0.1251055, 4)} & \Sexpr{round(-8.784369e-17, 4)} \\
			\Sexpr{round(-0.51849220, 4)} & \Sexpr{round(-0.2758386, 4)} & \Sexpr{round(-1.001942e-16, 4)}
			\end{bmatrix*}, &
    \text{\textbf{G:}}
    \begin{bmatrix*}[r]
			\Sexpr{round(-0.7520247, 4)} &  \Sexpr{round(0.03755489, 4)} & \Sexpr{round(1.076312e-16, 4)} \\
			\Sexpr{round(0.6739168, 4)} &  \Sexpr{round(0.23802000, 4)} & \Sexpr{round(1.076312e-16, 4)} \\
			\Sexpr{round(0.4589629, 4)} & \Sexpr{round(-0.74738468, 4)} & \Sexpr{round(1.076312e-16, 4)}
	\end{bmatrix*}
\end{align}
<<analiza_koresp_zadanie2, echo=F, warning=F, fig.cap="Porównanie wykresu analizy korespondencji z naszej funkcji (na górze) z wykresem funkcji wbudowanej (na dole), dla danych z zadania 2.">>=
  par(mfrow=c(2, 1))
analiza.kores(data, columnsName=c('do lat 35', 'od 36 do 55', 'powyżej 55'), 
rowsName=c('Ibuprom', 'Apap', 'Paracetamol', 'Ibuprofen', 'Panadol'))
plot(ca(data))
@
Wyznaczyliśmy inercję całkowitą, korzystając ze wzoru (\ref{eq:inercja}). Uzyskaliśmy następującą wartość:
<<echo=F>>=
dataA <- matrix(c(0.34298526, -0.26457513, -0.16201852,
0.03496029,  0.07416198, -0.18165902,
-0.08249579, -0.05000000,  0.22453656,
-0.33541020,  0.31622777,  0.06454972,
0.13023647, -0.16225573,  0.03938632), 
nrow=5, byrow = TRUE)
@
<<>>=
dataA
sum(diag(dataA %*% t(dataA)))
@
\subsubsection{Wnioski}
Naszą analizę rozpoczęliśmy od przeprowadzenia dwóch testów niezależności (Fishera i chi{\dywiz}kwadrat), na poziomie istotności $\alpha=0.01$. Tak jak w przypadku poprzedniego zadania, p{\dywiz}wartość uzyskana z testu Fishera wynosi dokładnie tyle samo co, dla zmiennych z poprzedniego zadania, więc jeszcze bardziej utwierdza nas to w przekonaniu, że założenia do przeprowadzenia tego testu (jak występujące zera w odpowiedziach) nie są spełnione. P{\dywiz}wartość z testu chi{\dywiz}kwadrat jest równa praktycznie równa zero, więc ponownie mamy podstawy do odrzucenia hipotezy o niezależności $H_0$, więc zakładamy, że nasze zmienne nie są niezależne, czyli badanie współzmienności jest uzasadnione.

W tym przypadku wyznaczone współczynniki mają dość spory rozrzut. Współczynnik Goodmana i Kruskala (z wykładu uznany za ważniejszy od innych) dał wartość w przybliżeniu 0.35. Patrząc na pozostałe współczynniki, najmniejszą wartość dał współczynnik T{\dywiz}Czurpowa ($\approx0.45$), a największą wartość \pauza współczynnik $\varphi$ ($\approx0.76$). Możemy uznać, że nasze zmienne charakteryzują się przeciętną albo wysoką współzmiennością.

Następnie przeszliśmy do analizy korespondencji. Procenty zmienności sumują się prawie do 100\% więc relacje są dobrze przedstawione. Z wykresu możemy odczytać, że w przypadku bólu osoby powyżej 55 roku życia stosunkowo najczęściej sięgają po ,,Paracetamol'', osoby do lat 35 po ,,Ibuprom'', a osoby między 36, a 55 rokiem życia, po ,,Ibuprofen'', gdyż odpowiednio te punkty na wykresie znajdują się najbliżej siebie.

Podobnie jak w poprzednim zadaniu współczynniki zmienności i analizę korespondencji wykonaliśmy, korzystając z naszej funkcji oraz funkcji wbudowanych w pakiecie R i uzyskaliśmy takie same wyniki.

Wartość inercji całkowitej jest równa w przybliżeniu 0.57, jest to znacznie większa wartość niż w zadaniu pierwszym (\ref{section:3.1.5}), co jest zgodne z intuicją, ponieważ porównując wykres (\ref{fig:analiza_koresp_zadanie2}) z wykresami (\ref{fig:analiza_koresp_m1a}), (\ref{fig:analiza_koresp_m2a}), (\ref{fig:analiza_koresp_m1b}), (\ref{fig:analiza_koresp_m1c}) widzimy, że wartości na osiach są znacznie większe, więc rozrzut punktów jest większy.
\subsection{Zadanie 3}
W tym zadaniu należało obliczyć odpowiednią miarę współzmienności zmiennych \emph{Wynagrodzenie} i \emph{Stopień zadowolenia z pracy} oraz przeprowadzić analizę korespondencji.
\subsubsection{Dane do analizy}
Będziemy rozważać następujące dane:
\begin{table}[h!]
	\begin{center}
		\begin{tabular}{cccccc}
			\hline
			\multicolumn{6}{c}{Stopień zadowolenia z pracy}\\\hline
			Wynagrodzenie & b. niezadow. & niezadow. & zadow. & b. zadow. & Suma \\\hline
			$< 6000$ & 32 & 44 & 60 & 70 & 206 \\
			6000 \pauza 15000 & 22 & 38 & 104 & 125 & 289 \\
			15000 \pauza 25000 & 13 & 48 & 61 & 113 & 235 \\
			$> 25000$ & 3 & 18 & 54 & 96 & 171 \\\hline
			Suma & 62 & 108 & 319 & 412 & 901 \\\hline
		\end{tabular}
	\end{center}
	\caption{Tablica dwudzielcza rozważanych danych}
	\label{tab:tabela8}
\end{table}
<<>>=
data2 <- matrix(c(32, 44, 60, 70,
		22, 38, 104, 125,
		13, 48, 61, 113,
		3, 18, 54, 96), byrow=T, nrow=4)
dimnames(data2) <- list(c('<6000', '6000 - 15000', '15000 - 25000', '> 25000'),
				c('b. niezadow', 'niezadow.', 'zadow.', 'b. zadow.'))
data2
@
\subsubsection{Analiza niezależności}
Przeprowadziliśmy test (\verb|chisq.test|), którego celem jest zweryfikowanie hipotezy o niezależności wysokości otrzymywanego wynagrodzenia i stopnia zadowolenia z pracy.\newline\noindent

Hipotezy dla testu:
\begin{itemize}[label=$\bullet$]
	\item $H_0$: \emph{Wynagrodzenie} i \emph{Stopień zadowolenia z pracy} są niezależne
	\item $H_1$: \emph{Wynagrodzenie} i \emph{Stopień zadowolenia z pracy} nie są niezależne
\end{itemize}
<<>>=
chisq.test(data2)$p.value
@

Uzyskaliśmy p{\dywiz}wartość dla testu chi \dywiz kwadrat równą praktycznie zero, więc możemy założyć, że hipoteza zerowa $H_0$ jest fałszywa, zatem zakładamy, że wynagrodzenie i stopnień zadowolenia z pracy nie są niezależne.

\subsubsection{Miara współzmienności}
W tym przypadku błędem byłoby wyznaczanie miar współzmienności, analogicznie jak w zadaniach 1. i 2. ((\ref{section:3.1}) i (\ref{section:3.2})), ponieważ nasze dane mają charakter uporządkowany.\newline\noindent
W przypadku zmiennych uporządkowanych, stosujemy inne miary współzmienności \pauza w tym zadaniu wyznaczymy miarę $\gamma$.
<<>>=
GoodmanKruskalGamma(data2)
@
\subsubsection{Analiza korespondencji}
Poniżej przedstawiliśmy macierze, które uzyskaliśmy z naszej funkcji, w celu przeprowadzenia analizy korespondencji. Wartości w macierzy zaokrągliliśmy do czwartego miejsca po przecinku:
\begin{align}
	  \text{\textbf{P:}}
	  \begin{bmatrix*}[r]
				\Sexpr{round(0.035516093, 4)} & \Sexpr{round(0.04883463, 4)} & \Sexpr{round(0.06659267, 4)} & \Sexpr{round(0.07769145, 4)} \\
				\Sexpr{round(0.024417314, 4)} & \Sexpr{round(0.04217536, 4)} & \Sexpr{round(0.11542730, 4)} & \Sexpr{round(0.13873474, 4)} \\
				\Sexpr{round(0.014428413, 4)} & \Sexpr{round(0.05327414, 4)} & \Sexpr{round(0.06770255, 4)} & \Sexpr{round(0.12541620, 4)} \\
				\Sexpr{round(0.003329634, 4)} & \Sexpr{round(0.01997780, 4)} & \Sexpr{round(0.05993341, 4)} & \Sexpr{round(0.10654828, 4)}
		\end{bmatrix*}, &
    \text{\textbf{A:}}
    \begin{bmatrix*}[r]
				\Sexpr{round(0.133203867, 4)} &  \Sexpr{round(0.05819913, 4)} & \Sexpr{round(-0.015805310, 4)} & \Sexpr{round(-0.07753757, 4)} \\
				\Sexpr{round(-0.003183739, 4)} & \Sexpr{round(-0.04579816, 4)} & \Sexpr{round(0.051097476, 4)} & \Sexpr{round(-0.01341809, 4)} \\
				\Sexpr{round(-0.040991673, 4)} &  \Sexpr{round(0.05039544, 4)} & \Sexpr{round(-0.045963056, 4)} &  \Sexpr{round(0.02475694, 4)} \\
				\Sexpr{round(-0.094008538, 4)} & \Sexpr{round(-0.06341766, 4)} &, \Sexpr{round( 0.004801831, 4)} &  \Sexpr{round(0.07352502, 4)}
	\end{bmatrix*} \\
    \text{\textbf{R:}}
    \begin{bmatrix*}[r]
\Sexpr{round(0.2286349, 4)} & \Sexpr{round(0.3207547, 4)} & \Sexpr{round(0.2608213, 4)} & \Sexpr{round(0.1897891, 4)}
\end{bmatrix*}, &
    \text{\textbf{C:}}
    \begin{bmatrix*}[r]
    \Sexpr{round(0.07769145, 4)} & \Sexpr{round(0.16426193, 4)} & \Sexpr{round(0.30965594, 4)} & \Sexpr{round(0.44839068, 4)}
\end{bmatrix*} \\
    \text{\textbf{F:}}
    \begin{bmatrix*}[r]
				\Sexpr{round(-0.34543921, 4)} & \Sexpr{round(-0.002507134, 4)} &  \Sexpr{round(0.02175994, 4)} & \Sexpr{round(3.141145e-17, 4)} \\
				\Sexpr{round(0.02998446, 4)} & \Sexpr{round(-0.117785342, 4)} & \Sexpr{round(-0.02235324, 4)} & \Sexpr{round(3.141145e-17, 4)} \\
				\Sexpr{round(0.04182549, 4)} & \Sexpr{round( 0.156690466, 4)} & \Sexpr{round(-0.01811548, 4)} & \Sexpr{round(3.141145e-17, 4)} \\
				\Sexpr{round(0.30798818, 4)} & \Sexpr{round(-0.013250444, 4)} &  \Sexpr{round(0.03646010, 4)} & \Sexpr{round(3.141145e-17, 4)}
	\end{bmatrix*}, &
    \text{\textbf{G:}}
    \begin{bmatrix*}[r]
			\Sexpr{round(-0.5943912, 4)} & \Sexpr{round(-0.0923127, 4)} &  \Sexpr{round(0.04549626, 4)} & \Sexpr{round(-3.141145e-17, 4)} \\
			\Sexpr{round(-0.2049741, 4)} & \Sexpr{round(0.1746038, 4)} & \Sexpr{round(-0.02887773, 4)} & \Sexpr{round(-3.141145e-17, 4)} \\
			\Sexpr{round(0.0263079, 4)} & \Sexpr{round(-0.1221969, 4)} & \Sexpr{round(-0.02260959, 4)} & \Sexpr{round(-3.141145e-17, 4)} \\
			\Sexpr{round(0.1599100, 4)} &  \Sexpr{round(0.0364195, 4)} & \Sexpr{round(0.01831000, 4)} & \Sexpr{round(-3.141145e-17, 4)}
	\end{bmatrix*}
\end{align}

<<analiza_koresp_zadanie3, echo=F, warning=F, fig.cap="Porównanie wykresu analizy korespondencji z naszej funkcji (na górze) z wykresem funkcji wbudowanej (na dole), dla danych z zadania 3.">>=
par(mfrow=c(2, 1))
analiza.kores(data2, columnsName=c('b. niezadow', 'niezadow.', 'zadow.', 'b. zadow.'), 
rowsName=c('<6000', '6000 - 15000', '15000 - 25000', '> 25000'))
plot(ca(data2))
@
Wyznaczyliśmy inercję całkowitą, korzystając ze wzoru (\ref{eq:inercja}). Uzyskaliśmy następującą wartość:
<<echo=F>>=
data2A <- matrix(c(0.133203867,  0.05819913, -0.015805310, -0.07753757,
-0.003183739, -0.04579816,  0.051097476, -0.01341809,
-0.040991673,  0.05039544, -0.045963056,  0.02475694,
-0.094008538, -0.06341766,  0.004801831,  0.07352502), 
nrow=4, byrow = TRUE)
@
<<>>=
data2A
sum(diag(data2A %*% t(data2A)))
@
\subsubsection{Wnioski}
Naszą analizę rozpoczęliśmy od przeprowadzenia testu niezależności chi{\dywiz}kwadrat, na poziomie istotności $\alpha=0.01$ i uzyskaliśmy p{\dywiz}wartość równą w przybliżeniu $4.9\cdot10^{-8}$ więc mamy podstawy do odrzucenia hipotezy o niezależności $H_0$ i przyjęcia hipotezy alternatywnej $H_1$, czyli zakładamy, że nasze zmienne nie są niezależne. Więc rozsądnym jest badanie współzmienności zmiennych. W tym przypadku mamy zmienne porządkowe (\emph{Wynagrodzenie} i \emph{Stopień zadowolenia z pracy}), więc w tym przypadku zastosowaliśmy inny współczynnik ($\gamma$ Goodmana i Kruskala) niż w poprzednich zadaniach. Tym razem skorzystaliśmy tylko z funkcji wbudowanej w pakiet R i uzyskaliśmy wartość w przybliżeniu równą 0.22, przez co możemy wyciągnąć wniosek, że nasze zmienne charakteryzuje słaba współzmienność.

Na rysunku (\ref{fig:analiza_koresp_zadanie3}) przedstawiliśmy wykresy analizy korespondencji. Punkty są dość równomiernie rozrzucone na wykresie, za wyjątkiem punktów reprezentujących osoby ankietowane, zarabiające między 6000, a 15000 i osoby zadowolone z pracy, które praktycznie nachodzą na siebie. Możemy więc wyciągnąć wniosek że dużo osób, których zarobki mieszczą się w przedziale 6000 \ppauza 15000 jest zadowolonych z pracy. Osoby ankietowane, zarabiające najwięcej, najczęściej wybierały pozytywne odpowiedzi jeśli chodzi o zadowolenie z pracy. Punkt reprezentujący odpowiedzi ,,bardzo niezadowolony'' lub ,,niezadowolony'' znajduje się najbliżej osób zarabiających mniej niż 6000, więc wśród osób ankietowanych, stosunkowo najczęściej osoby najmniej zarabiające, zaznaczały takie odpowiedzi.
Analizę korespondencji wykonaliśmy korzystając z naszej funkcji oraz funkcji wbudowanej i uzyskaliśmy taki sam wykres.

Wartość inercji całkowitej jest równa w przybliżeniu 0.058.
\end{document}